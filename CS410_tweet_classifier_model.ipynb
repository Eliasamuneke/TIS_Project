{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "\n",
        "# Load the dataset from CSV using pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('Tweets.csv')\n",
        "data.head()  # Display the first few rows of the dataset"
      ],
      "metadata": {
        "id": "lCjggVkja-p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Model**"
      ],
      "metadata": {
        "id": "T60cN13SbFnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dnmCJxBazIx"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels for negative, positive, neutral\n",
        "\n",
        "# Define our dataset class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.tweets[idx])  # Ensure the text is a string\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return {'text': text, 'label': label}\n",
        "\n",
        "# Tokenize and create DataLoader for training set\n",
        "train_tweets = train_df['selected_text'].tolist()\n",
        "train_labels = train_df['sentiment'].map({'negative': 0, 'positive': 1, 'neutral': 2}).tolist()\n",
        "train_dataset = TweetDataset(train_tweets, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Tokenize and create DataLoader for testing set\n",
        "test_tweets = test_df['text'].tolist()\n",
        "test_labels = test_df['sentiment'].map({'negative': 0, 'positive': 1, 'neutral': 2}).tolist()\n",
        "test_dataset = TweetDataset(test_tweets, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Fine-tuning parameters\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tune the model with time logging\n",
        "print('Fine-tuning BERT on tweets... Starting Epochs\\n')\n",
        "start_time = time.time()  # Record the start time\n",
        "# print(\"Start time:\", start_time)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        # Filter out NaN values from 'batch['text']'\n",
        "        texts = [text for text in batch['text'] if pd.notna(text)]\n",
        "\n",
        "        if not texts:\n",
        "            # Skip empty batches\n",
        "            print(\"Empty batch skipped\")\n",
        "            continue\n",
        "\n",
        "        # print('Texts in the batch:', batch['text'])  # Display the texts in each batch\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "        labels = torch.tensor(batch['label'])\n",
        "        inputs.to(device)\n",
        "        labels.to(device)\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(labels.cpu(), \"Current time elapsed:\", time.time() - start_time, end='\\r')  # Display the labels for each batch\n",
        "        # print(\"Current time elapsed:\", time.time() - start_time)\n",
        "\n",
        "end_time = time.time()  # Record the end time\n",
        "elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "print(f'Finished Fine-tuning. Elapsed Time: {elapsed_time/60} minutes and {elapsed_time%60} seconds\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the testing set with time logging\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_true_labels = []\n",
        "\n",
        "print('Evaluating BERT on tweets... Starting Predictions\\n')\n",
        "start_time = time.time()  # Record the start time\n",
        "# print(\"Start time:\", start_time)\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        # Filter out NaN values from 'batch['text']'\n",
        "        texts = [text for text in batch['text'] if pd.notna(text)]\n",
        "\n",
        "        if not texts:\n",
        "            # Skip empty batches\n",
        "            print(\"Empty batch skipped\")\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "        labels = torch.tensor(batch['label'])\n",
        "        inputs.to(device)\n",
        "        labels.to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        test_predictions.extend(predictions.tolist())\n",
        "        test_true_labels.extend(labels.tolist())\n",
        "        print(labels.cpu(), \"Current time elapsed:\", time.time() - start_time, end='\\r')  # Display the labels for each batch\n",
        "        # print(\"Current time elapsed:\", time.time() - start_time)\n",
        "\n",
        "end_time = time.time()  # Record the end time\n",
        "elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "print(f'Finished Predictions. Elapsed Time: {elapsed_time/60} minutes and {elapsed_time%60} seconds\\n')\n"
      ],
      "metadata": {
        "id": "HARHjHhhbN7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print evaluation metrics\n",
        "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "classification_report_str = classification_report(test_true_labels, test_predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:\\n', classification_report_str)"
      ],
      "metadata": {
        "id": "TOLGk2VRa5Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-t2BlRgwAxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}